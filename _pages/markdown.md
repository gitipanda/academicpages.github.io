---
layout: archive
title: 
permalink: /markdown/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}



<span style="font-size:18pt;">**Publication**</span>
* Fast Global Convergence in Random Sensing Problems for Low-rank Matrix with Random Initialization (working paper). Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang, 2021.

* Robust low-rank matrix recovery by Riemannian subgradient method (working paper). Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang, 2021.

* [<span style="color:#3da2ce">*Asymptotic escape of spurious fixed points on the low-rank matrix manifold*</span>](https://arxiv.org/abs/2107.09207). Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang, submitted to Communications of the AMS, 2021.

* [<span style="color:#3da2ce">*Fast Global Convergence for Low-rank Matrix Recovery via Riemannian Gradient Descent with Random Initialization*</span>](https://arxiv.org/abs/2012.15467). Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang, submitted to Foundations of Computational Mathematics (FoCM), 2021.

* [<span style="color:#3da2ce">*Analysis of Asymptotic Escape of Strict Saddle Sets in Manifold Optimization*</span>](https://epubs.siam.org/doi/abs/10.1137/19M129437X?mobileUi=0&). Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang, SIAM Journal on Mathematics of Data Science, 2020, 2(3): 840-871.

* Nonconvex Optimization for Low-rank Matrix Related Problems. Zhenzhen Li, Ph.D. Thesis, Hong Kong University of Science and Technology, 2020.

*  [<span style="color:#3da2ce">*Towards the Optimal Construction of a Loss Function without Spurious Local Minima for Solving Quadratic Equations*</span>](https://ieeexplore.ieee.org/document/8918236). Zhenzhen Li, Jian-feng Cai, Ke Wei, IEEE Transactions on Information Theory, 66(5): 3242--3260, 2020.

<span style="font-size:18pt;">**Projects**</span>

<p float="left">
  <img src="/images/rPCA.png" width="300" align="left" style="margin-right: 8em"/>
</p> 
<br/> <em>Robust low-rank matrix recovery by Riemannian subgradient method.</em> 
<br/>
In this project, we explore theoretical guarantee for Riemannian subgradient descent method in solving the robust PCA problem. The numerical experiments show that our method is 20% faster than prior methods such as:  AccAltProj, AltProj and GD based method.
<br/><br/><br/><br/>


<p float="left">
  <img src="/images/es1.png" width="200" align="left" style="margin-right: 1em"/>  
  <img src="/images/es2.png" width="200" align="left" style="margin-right: 2em"/> 
</p>
<em>Analysis of Asymptotic Escape of Strict Saddle Sets in Manifold Optimization.</em>
<br/>
In this project, we explore how Riemannian gradient descent method will escape strict saddle sets asymptotically, and converge to local minimizers and shows examples in phase retrieval and variational eigen problems.


<p float="left">
  <img src="/images/dn1.png" width="200" align="left" style="margin-right: 1em"/>  
  <img src="/images/dn2.png" width="200" align="left" style="margin-right: 2em"/> 
</p>
<em>Real-time noise level detection and denoising</em>
<br/>
In this project, we develop noise level detection and establish a real-time videos denoising for Wechat video group. We use wavelet features and data-driven dictionary learning methods. Our package meets the industrial standard that can process a single frame within 1/100s and with satisfied video quality.



<!---
<p float="left">
  <img src="/images/es1.png" width="150" align="left" style="margin-right: 1em"/>  
  <img src="/images/es2.png" width="150" style="float:left; margin-right: 2em;"/> 
</p>
<em>Analysis of Asymptotic Escape of Strict Saddle Sets in Manifold Optimization.</em>
In this project, we explore how Riemannian gradient descent method will escape strict saddle sets asymptotically, and converge to local minimizers and shows examples in phase retrieval and variational eigen problems.
<br />
<img src="/images/es2.png" align="left" width="200" style="margin-right: 2em"/> 
[image](/es1.png){: style="float: left"; margin-right: 2em; height="10%" width="10%"}
![](/images/es1.png){: height="100" width="100"} ![](/images/es2.png){: style="float: left"; margin-right: 2em; height="100" width="100"} Analysis of Asymptotic Escape of Strict Saddle Sets in Manifold Optimization. In this project, we explore how Riemannian gradient descent method will escape strict saddle sets asymptotically, and converge to local minimizers and shows examples in phase retrieval and variational eigen problems.
-->
